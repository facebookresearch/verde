{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from math import ceil\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "display(HTML(\"<style>.container {width:90% !important;}</style>\"))\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from operator import itemgetter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91b1dcc6",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = '/used/with/slurm/runs'\n",
    "PATHS = ['/path/to/train/logs']\n",
    "df_save_path = None \n",
    "\n",
    "PREFIX_INDICATOR = \"valid\"\n",
    "XPS_PATHS = []\n",
    "\n",
    "VAR_ARGS = set()\n",
    "ALL_ARGS = {}\n",
    "DATA_LOAD_ARGS = ['reload_size','batch_load','shuffle','reuse','num_reuse_samples','times_reused','output_int_base','correctQ','balanced_base','add_unred_perc']\n",
    "MODEL_ARGS = ['max_output_len','max_len','xav_init','gelu_activation','norm_attention','dropout','attention_dropout','use_circreg','reg_value']\n",
    "EVAL_ARGS = ['eval_only','eval_from_exp','eval_data','eval_verbose','eval_verbose_print','stopping_criterion','validation_metrics']\n",
    "RUN_ARGS = ['fp16','amp','debug_slurm','debug','cpu','local_rank','master_port','windows','nvidia_apex','is_slurm_job','node_id','global_rank','world_size']\n",
    "UNWANTED_ARGS = DATA_LOAD_ARGS + MODEL_ARGS + EVAL_ARGS + RUN_ARGS + ['dim_red','data_cols','dense_cols']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patterns:\n",
    "    RuntimeError = \"RuntimeError:\"\n",
    "    CudaOOM = \"CUDA out of memory\"\n",
    "    Terminated = \"Exited with exit code 1\"\n",
    "    Forced = \"Force Terminated\"\n",
    "    Signal10 = \"Signal handler called with signal 10\"\n",
    "    Signal15 = \"Signal handler called with signal 15\"\n",
    "    EpochStart = \"============ Starting epoch\"\n",
    "    EpochEnd = \"============ End of epoch\"\n",
    "    EpochLog = '__log__:'\n",
    "    IterationLog = \"- LR:\"\n",
    "    Cancelled = \"CANCELLED AT\"\n",
    "    NodeFailure = \"DUE TO NODE FAILURE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cd4543a",
   "metadata": {},
   "source": [
    "# Parsing Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for PATH_ENV in PATHS:\n",
    "    XPS_PATHS += [os.path.join(PATH_ENV, name) for name in os.listdir(PATH_ENV)]\n",
    "print(len(XPS_PATHS),\"experiments found\")\n",
    "\n",
    "pickled_xp = 0\n",
    "for path in XPS_PATHS:\n",
    "    pa = os.path.join(path, 'params.pkl')\n",
    "    if not os.path.exists(pa):\n",
    "        print(\"Unpickled experiment: \", path)\n",
    "        continue\n",
    "    pk = pickle.load(open(pa,'rb'))\n",
    "    ALL_ARGS.update(pk.__dict__)\n",
    "    pickled_xp += 1\n",
    "print(pickled_xp, \"pickled experiments found\")\n",
    "print()\n",
    "\n",
    "for path in XPS_PATHS:\n",
    "    pa = os.path.join(path, 'params.pkl')\n",
    "    if not os.path.exists(pa):\n",
    "        continue\n",
    "    pk = pickle.load(open(pa,'rb'))\n",
    "    for key,value in ALL_ARGS.items():\n",
    "        if key in pk.__dict__ and np.all(value == pk.__dict__[key]):\n",
    "            continue\n",
    "        if key not in UNWANTED_ARGS:\n",
    "            VAR_ARGS.add(key)\n",
    "            \n",
    "            \n",
    "print(\"common args\")\n",
    "for key in ALL_ARGS:\n",
    "    if key not in UNWANTED_ARGS and key not in VAR_ARGS:\n",
    "        print(key,\"=\", ALL_ARGS[key])\n",
    "print()\n",
    "            \n",
    "print(len(VAR_ARGS),\" variables params out of\", len(ALL_ARGS))\n",
    "print(VAR_ARGS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fb409a2",
   "metadata": {},
   "source": [
    "# Useful Functions to Parse Experiment Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stderr(xp_path):\n",
    "    dirs = xp_path.split('/')\n",
    "    EXP_ENV, xp = dirs[-2], dirs[-1]\n",
    "    res = {\"env\": EXP_ENV, \"xp\": xp, \"stderr\": False, \"log\": False, \"error\": False}\n",
    "    stderr_file = os.path.join(WORKDIR or os.path.expanduser(\"~\"), 'workdir/'+EXP_ENV+'/*/'+xp+'.stderr')\n",
    "    nb_stderr =len(glob.glob(stderr_file))\n",
    "    if nb_stderr > 1:\n",
    "        print(\"duplicate stderr\", EXP_ENV, xp)\n",
    "        return res\n",
    "    \n",
    "    for name in glob.glob(stderr_file):\n",
    "        with open(name, 'rt') as f:\n",
    "            res.update({\"stderr\": True, \"runtime_errors\": [], \"oom\": False, \"terminated\": False, \"forced\": False, \"cancelled\": False})\n",
    "            \n",
    "            for line in f:\n",
    "                if line.find(Patterns.RuntimeError) >= 0:\n",
    "                    res[\"error\"] = True\n",
    "                    res[\"runtime_errors\"].append(line)\n",
    "                if line.find(Patterns.CudaOOM) >= 0:\n",
    "                    res[\"oom\"] = cuda \n",
    "                if line.find(Patterns.Terminated) >=0:\n",
    "                    res[\"terminated\"] = True\n",
    "                if line.find(Patterns.Forced) >=0:\n",
    "                    res[\"forced\"] = True\n",
    "                if (line.find(Patterns.Cancelled) >=0) and (line.find(Patterns.Requeue)<0) and (line.find(Patterns.NodeFailure)< 0):\n",
    "                    res[\"cancelled\"] = True\n",
    "                if line.find('NaN detected')>=0:\n",
    "                    break\n",
    "\n",
    "            if len(res[\"runtime_errors\"]) > 0 and not cuda:    \n",
    "                print(stderr_file,\"runtime error no oom\")\n",
    "    return res\n",
    "\n",
    "def read_params(res, xp_path):\n",
    "    pa = os.path.join(xp_path, 'params.pkl')\n",
    "    if not os.path.exists(pa):\n",
    "        print(\"pickle\", pa, \"not found\")\n",
    "        return res\n",
    "    pk = pickle.load(open(pa,'rb'))\n",
    "    for key in VAR_ARGS:\n",
    "        res[key] = pk.__dict__[key] if key in pk.__dict__ else None\n",
    "    for key in [\"batch_size\", \"N\", \"hamming\", \"Q\", \"sigma\"]:\n",
    "        if key not in VAR_ARGS:\n",
    "            res[key] = ALL_ARGS[key]\n",
    "    return res\n",
    "            \n",
    "def read_train_log(res, xp_path, max_epoch=None):\n",
    "    pa = os.path.join(xp_path, 'train.log')\n",
    "    if not os.path.exists(pa):\n",
    "        return res\n",
    "    res.update({\"log\": True, \"nans\": False, \"curr_epoch\": -1, \"nonzeros_epoch\": 9999, \"nb_sig10\": 0, \"nb_sig15\": 0, \"train_loss\": [], \"val_loss\": []})\n",
    "    with open(pa, 'rt') as f:\n",
    "        train_acc = []\n",
    "        nonzeros_not_matched = 0\n",
    "        for line in f:\n",
    "            try:\n",
    "                if line.find('NaN detected')>=0:\n",
    "                    res[\"nans\"] = True\n",
    "                    break\n",
    "                if line.find(Patterns.Signal10) >= 0:\n",
    "                    nb_sig10 += 1\n",
    "                if line.find(Patterns.Signal15) >= 0:\n",
    "                    nb_sig15 += 1\n",
    "\n",
    "                if line.find('Nonzero bits not identified. ')>=0:\n",
    "                    nonzeros_not_matched += 1\n",
    "                if line.find(Patterns.EpochStart) >=0:\n",
    "                    curr_epoch = int(line.split('epoch ')[1].split()[0])\n",
    "                    if curr_epoch == max_epoch: break\n",
    "                    res[\"curr_epoch\"] = curr_epoch\n",
    "                    nonzeros_not_matched = 0\n",
    "                if line.find(' - Saving checkpoint to ') >=0:\n",
    "                    if nonzeros_not_matched != 4:\n",
    "                        res[\"nonzeros_epoch\"] = min(res[\"nonzeros_epoch\"], curr_epoch)\n",
    "                if line.find(Patterns.EpochEnd) >=0:\n",
    "                    curr_epoch = int(line.split('epoch ')[1].split()[0])\n",
    "                    if curr_epoch != res[\"curr_epoch\"]:\n",
    "                        print(\"epoch mismatch\", curr_epoch, \"in\", xp_path)\n",
    "\n",
    "                if line.find(Patterns.IterationLog) >=0:\n",
    "                    loss = line.split(\"LOSS: \")[1].split(' - ')[0].split('||')\n",
    "                    if line.find('ACC1: ') >=0:\n",
    "                        acc = loss[1].split('ACC1: ')[1].strip()\n",
    "                        train_acc.append(None if acc == 'nan' else float(acc))\n",
    "                    loss = loss[0].strip()\n",
    "                    res[\"train_loss\"].append(None if loss == 'nan' else float(loss)) \n",
    "                if line.find(Patterns.EpochLog) > 0:\n",
    "                    res[\"val_loss\"].append(float(line.split('valid_xe_loss\\\": ')[1].split(',')[0].split('}')[0]))\n",
    "                                \n",
    "            except Exception as e:\n",
    "                print(e, \"exception in\", xp_path)\n",
    "                continue\n",
    "                \n",
    "        if len(train_acc) > 5:\n",
    "            res[\"train_acc\"] = train_acc\n",
    "            res[\"Max acc\"] = np.mean(train_acc[-5:])\n",
    "        if res[\"nonzeros_epoch\"] == 9999:\n",
    "            res[\"nonzeros_epoch\"] = -1\n",
    "        res[\"best_xe_loss\"] = min(res[\"val_loss\"])\n",
    "        res[\"last_xe_loss\"] = res[\"val_loss\"][-1]\n",
    "        \n",
    "    return res\n",
    "\n",
    "def read_secret_rec(res, xp_path):\n",
    "    res.update({\"success_epoch\": -1, \"success\": False})\n",
    "    result_epoch = res[\"curr_epoch\"]\n",
    "    while result_epoch >= 0:\n",
    "        pa = os.path.join(xp_path, f'secret_recovery_{result_epoch}.pkl')\n",
    "        if os.path.exists(pa):\n",
    "            try:\n",
    "                pk = pickle.load(open(pa,'rb'))\n",
    "                if type(pk) != dict:\n",
    "                    pk = pk.__dict__\n",
    "                res['success_methods'] = pk['success']\n",
    "                if len(pk['success']) > 0:\n",
    "                    res[\"success_epoch\"] = result_epoch\n",
    "                    res[\"success\"] = True\n",
    "                if 'partial_success' in pk:\n",
    "                    res[\"nonzeros_epoch\"] = result_epoch\n",
    "            except:\n",
    "                print('error reading secret recovery pickle')\n",
    "        \n",
    "        else:\n",
    "            if result_epoch != res[\"curr_epoch\"]:\n",
    "                print(\"secret recovery pickle\", xp_path, \"not found\")\n",
    "        result_epoch -= 1\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabad8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "failed = {}\n",
    "for xp_path in XPS_PATHS:\n",
    "    res = read_stderr(xp_path) \n",
    "    res = read_params(res, xp_path)\n",
    "    res = read_train_log(res, xp_path, None)\n",
    "    data.append(read_secret_rec(res, xp_path))\n",
    "    if res[\"error\"]:\n",
    "        key = str(res[\"N\"]) +\" ; \"+ str(res[\"batch_size\"])\n",
    "        if key in failed:\n",
    "            failed[key] +=1\n",
    "        else:\n",
    "            failed[key] = 1\n",
    "print(failed)\n",
    "print(len(data), \"experiments read\")\n",
    "print(len([d for d in data if d[\"stderr\"] is False]),\"stderr not found\")\n",
    "print(len([d for d in data if d[\"error\"] is True]),\"runtime errors\")\n",
    "print(len([d for d in data if \"oom\" in d and d[\"oom\"] is True]),\"oom errors\")\n",
    "print(len([d for d in data if \"terminated\" in d and d[\"terminated\"] is True]),\"exit code 1\")\n",
    "print(len([d for d in data if \"forced\" in d and d[\"forced\"] is True]),\"Force Terminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(f,g):\n",
    "    return lambda x : f(g(x))\n",
    "\n",
    "def print_table(data, args, sort=False):\n",
    "    res = []\n",
    "    for d in data:\n",
    "        line = [d[v] if v in d else None for v in args]\n",
    "        res.append(line)\n",
    "    if sort:\n",
    "        res = sorted(res, key=compose(float,itemgetter(0)), reverse=True)\n",
    "    print(tabulate(res,headers=args,tablefmt=\"pretty\"))\n",
    "\n",
    "\n",
    "    \n",
    "def speed_table(data, args, indic, sort=False, percent=95):\n",
    "    res = []\n",
    "    for d in data:\n",
    "        \n",
    "        if indic in d:\n",
    "            line = [d[v] if v in d else None for v in args]\n",
    "            val= 1000\n",
    "            for i,v in enumerate(d[indic]):\n",
    "                if v >= percent and i < val:\n",
    "                    val = i\n",
    "                    \n",
    "            line.insert(1,val)\n",
    "            res.append(line)\n",
    "    e= args.copy()\n",
    "    e.insert(1,'first epoch')\n",
    "    if sort:\n",
    "        res = sorted(res, key=compose(float,itemgetter(1)), reverse=False)\n",
    "    print(tabulate(res,headers=e,tablefmt=\"pretty\"))\n",
    "\n",
    "def training_curve(data, indic, beg=0, end=-1, maxval=None, minval=None, smooth=1):\n",
    "    for d in data:\n",
    "        if indic in d:\n",
    "            if smooth != 1:\n",
    "                num_points = len(d[indic])//smooth\n",
    "                smoothed = np.empty(())\n",
    "                plt.plot([i*smooth for i in range(num_points)], np.mean(np.array(d[indic][:num_points*smooth]).reshape(-1, smooth), axis=1))\n",
    "            elif end == -1:\n",
    "                plt.plot(d[indic][beg:])\n",
    "            else:\n",
    "                plt.plot(d[indic][beg:end])\n",
    "    plt.ylim(minval,maxval)\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    plt.title(indic.replace(\"_\", \" \").title())\n",
    "    plt.show()\n",
    "    \n",
    "def filter_xp(xp, filt):\n",
    "    for f in filt:\n",
    "        if not f in xp:\n",
    "            return False\n",
    "        if not xp[f] in filt[f]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def xp_stats(data, splits, best_arg, best_value):\n",
    "    res_dic = {}\n",
    "    nb = 0\n",
    "    for d in data:\n",
    "        if d[best_arg] < best_value: continue\n",
    "        nb += 1\n",
    "        for s in splits:\n",
    "            if not s in d: continue\n",
    "            lib=s+':'+str(d[s])\n",
    "            if lib in res_dic:\n",
    "                res_dic[lib] += 1\n",
    "            else:\n",
    "                res_dic[lib]=1\n",
    "                \n",
    "    print()\n",
    "    print(f\"{nb} experiments with accuracy over {best_value}\")\n",
    "    for elem in sorted(res_dic):\n",
    "        print(elem,' : ',res_dic[elem])\n",
    "    print()\n",
    "\n",
    "\n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def94f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_filter ={} \n",
    "\n",
    "table_args = VAR_ARGS - set(['env_base_seed','secret_col','secret','master_addr','dump_path'])\n",
    "fdata = [d for d in data if filter_xp(d, xp_filter) is True]\n",
    "\n",
    "oomtab = [d for d in fdata if d[\"error\"] is True]\n",
    "print(f\"CUDA out of memory ({len(oomtab)})\")\n",
    "print_table(oomtab, table_args)\n",
    "\n",
    "forcetab = [d for d in fdata if 'forced' in d and d[\"forced\"] is True]\n",
    "print(f\"Forced terminations ({len(forcetab)})\")\n",
    "print_table(forcetab, table_args)\n",
    "\n",
    "unstartedtab = [d for d in fdata if \"curr_epoch\" in d and d[\"curr_epoch\"] < 0] \n",
    "print(f\"Not started ({len(unstartedtab)})\")\n",
    "print_table(unstartedtab, table_args)\n",
    "\n",
    "crypto = False\n",
    "runargs = [\"curr_epoch\", \"best_xe_loss\", \"last_xe_loss\", \"nans\", \"error\"]\n",
    "for v in table_args:\n",
    "    runargs.append(v)\n",
    "    \n",
    "runningtab = [d for d in fdata if \"curr_epoch\" in d and d[\"curr_epoch\"] >= 0] \n",
    "print(f\"Running experiments ({len(runningtab)})\")\n",
    "print_table(runningtab, runargs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d211e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "training_curve(fdata, \"val_loss\")\n",
    "training_curve(fdata, \"train_loss\")\n",
    "training_curve(fdata, \"train_acc\", smooth=10)\n",
    "# speed_table(runningtab, runargs, \"beam_acc\" , sort=True,percent=85)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "370ffe51",
   "metadata": {},
   "source": [
    "# Get experiment results as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75116dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(runningtab)\n",
    "print(df.shape)\n",
    "for h, methods, n_epoch in zip(df['hamming'], df['success_methods'], df['curr_epoch']):\n",
    "    print(h, n_epoch, methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ad152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def groupby_and_count(df, criterions, groupby_vars):\n",
    "    sub = df[[*groupby_vars, *criterions]]\n",
    "    gsub = sub.groupby(groupby_vars)\n",
    "    df_count = gsub.sum()\n",
    "    df_total = gsub.count()\n",
    "    \n",
    "    epochs, partial_epochs, partial = [], [], []\n",
    "    for h in np.unique(df['hamming']):\n",
    "        hdf = df[df['hamming'] == h]\n",
    "        success_epochs = list(hdf[hdf['success_epoch']!= -1]['success_epoch'])\n",
    "        nonzeros_epochs = list(hdf[hdf['nonzeros_epoch']!= -1]['nonzeros_epoch'])\n",
    "        partial.append(len(nonzeros_epochs))\n",
    "        epochs.append(','.join([str(ep) for ep in sorted(success_epochs)]))\n",
    "        partial_epochs.append(','.join([str(ep) for ep in sorted(nonzeros_epochs)]))\n",
    "    \n",
    "    for criterion in criterions:\n",
    "        df_count[criterion] = df_count[criterion].astype(\"int\").astype(\"str\") + \"/\" + df_total[criterion].astype(\"int\").astype(\"str\")\n",
    "    df_count['epochs'] = epochs\n",
    "    df_count['partial success'] = np.array(partial).astype(\"int\").astype(\"str\")\n",
    "    df_count['partial success'] = df_count['partial success'] + \"/\" + df_total[criterion].astype(\"int\").astype(\"str\")\n",
    "    df_count['nonzeros_epochs'] = partial_epochs\n",
    "    return df_count\n",
    "\n",
    "ended = groupby_and_count(df, [\"success\"], ['hamming', 'N'])\n",
    "pd.DataFrame(ended.unstack().transpose())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
